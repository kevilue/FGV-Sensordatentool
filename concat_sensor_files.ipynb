{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bd678dab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read configs (required!)\n",
    "\n",
    "import tomllib\n",
    "with open(\"settings.toml\", \"rb\") as f:\n",
    "    config = tomllib.load(f)\n",
    "\n",
    "with open(\"sensors.toml\", \"rb\") as f:\n",
    "    sensors = tomllib.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9a32ee59",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transformSensorFile(df_dict: dict, sensor_name: str, datetime_col=False):\n",
    "    \"\"\"Split the three existing columns into eight with sensor info, location and separate columns for time data.\"\"\"\n",
    "    df_dict[\"df\"].drop(df_dict[\"idxcol\"], axis=1, inplace=True)\n",
    "    if not datetime_col: df_dict[\"df\"][\"Datum\"] = df_dict[\"df\"][df_dict[\"timecol\"]].dt.date\n",
    "    else: df_dict[\"df\"][\"Datum\"] = df_dict[\"df\"][df_dict[\"timecol\"]]\n",
    "    df_dict[\"df\"][\"Jahr\"] = df_dict[\"df\"][df_dict[\"timecol\"]].dt.year\n",
    "    df_dict[\"df\"][\"Monat\"] = df_dict[\"df\"][df_dict[\"timecol\"]].dt.month\n",
    "    df_dict[\"df\"][\"Tag\"] = df_dict[\"df\"][df_dict[\"timecol\"]].dt.day\n",
    "    df_dict[\"df\"][\"Uhrzeit\"] = df_dict[\"df\"][df_dict[\"timecol\"]].dt.time\n",
    "    df_dict[\"df\"][\"Sensor\"] = sensor_name\n",
    "    df_dict[\"df\"][\"Standort\"] = sensors[sensor_name][\"location\"]\n",
    "    df_dict[\"df\"].drop(df_dict[\"timecol\"], axis=1, inplace=True)\n",
    "    df_dict[\"df\"].rename(columns={df_dict[\"tmpcol\"]: \"Temperatur\"}, inplace=True)\n",
    "    return df_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73ae90f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read excel files\n",
    "import pandas as pd\n",
    "import glob\n",
    "import re\n",
    "import time\n",
    "\n",
    "path_to_files = r\"C:\\Users\\kevin\\Documents\\Code\\angelverein\\Daten FGV\"\n",
    "save_path = \"all_data.csv\"\n",
    "\n",
    "def concat_sensor_files(path_to_files: str, save_path=None) -> None | pd.DataFrame :\n",
    "    \"\"\"Concatenate all given csv files collected from sensors into new ones.\"\"\"\n",
    "    data_paths = glob.glob(path_to_files+\"\\\\FGV_*.xlsx\", recursive=True)\n",
    "    print(\"Found\", len(data_paths), \"files\")\n",
    "\n",
    "    sensors_chunks = {key: [] for key in sensors.keys()}\n",
    "\n",
    "    if save_path is not None: stime = time.perf_counter()\n",
    "\n",
    "    for idx, file in enumerate(data_paths):\n",
    "        sensor_name = re.search(r\"FGV_\\d+\", file).group()\n",
    "        if not sensor_name in sensors.keys():\n",
    "            raise(NameError(f\"Trying to read sensor {sensor_name} which is not defined in sensors.toml\"))\n",
    "        print(\"Reading sensor\", sensor_name, f\"({idx+1}/{len(data_paths)})\")\n",
    "        df = pd.read_excel(file)\n",
    "\n",
    "        idxcol, timecol, tmpcol = None, None, None\n",
    "        for idx, col in enumerate(df.columns):\n",
    "            # print(idx, col)\n",
    "            if config[\"names\"][\"index_column\"] in col:\n",
    "                idxcol = col\n",
    "                # print(\"Found index column:\", col)\n",
    "            elif config[\"names\"][\"timestamp_column\"] in col:\n",
    "                timecol = col\n",
    "                # print(\"Found timestamp column:\", col)\n",
    "            elif config[\"names\"][\"temperature_column\"] in col:\n",
    "                tmpcol = col\n",
    "                # print(\"Found temperature column:\", col)\n",
    "            else: raise(IndexError(f\"Found unknown column: {col}\"))\n",
    "\n",
    "        df[timecol] = pd.to_datetime(df[timecol], format=config[\"formats\"][\"time_format\"])\n",
    "        df.sort_values(timecol, ascending=False, inplace=True)\n",
    "        df.dropna()\n",
    "        # sensors_chunks[sensor_name].append({\"df\": df, \"idxcol\": idxcol, \"timecol\": timecol, \"tmpcol\": tmpcol})\n",
    "        df = transformSensorFile({\"df\": df, \"idxcol\": idxcol, \"timecol\": timecol, \"tmpcol\": tmpcol}, sensor_name, datetime_col=True)[\"df\"]\n",
    "        sensors_chunks[sensor_name].append(df)\n",
    "\n",
    "    # Use topmost entry\n",
    "    searchfunc = lambda x: x[\"Datum\"].iloc[0]\n",
    "    # Sort chunks after newest newest entry\n",
    "    for key in sensors_chunks.keys():\n",
    "        sensors_chunks[key].sort(key=searchfunc, reverse=True) # Newest at top\n",
    "        for idx, x in enumerate(sensors_chunks[key]):\n",
    "            if save_path is not None: print(f\"{idx}: {searchfunc(x)}\", end=\" \")\n",
    "        if save_path is not None: print(f\"(Sensor {key})\")\n",
    "        sensors_chunks[key] = pd.concat(sensors_chunks[key])\n",
    "\n",
    "    all_sensors_chunks = [sensors_chunks[key] for key in sensors_chunks.keys()]\n",
    "    all_sensors_chunks = pd.concat(all_sensors_chunks)\n",
    "    # sensors_chunks should now contain all the sensor files, sorted for all sensors read with newest data at the top\n",
    "\n",
    "    if save_path is not None:\n",
    "        with open(save_path, \"w\") as f:\n",
    "            all_sensors_chunks.to_csv(f, index=False)\n",
    "\n",
    "        ttime = time.perf_counter() - stime\n",
    "        print(f\"Finished in {ttime:.2f}s\")\n",
    "\n",
    "    else: return all_sensors_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a535a73f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 16 files\n",
      "Reading sensor FGV_01 (1/16)\n",
      "Reading sensor FGV_02 (2/16)\n",
      "Reading sensor FGV_02 (3/16)\n",
      "Reading sensor FGV_03 (4/16)\n",
      "Reading sensor FGV_03 (5/16)\n",
      "Reading sensor FGV_04 (6/16)\n",
      "Reading sensor FGV_05 (7/16)\n",
      "Reading sensor FGV_06 (8/16)\n",
      "Reading sensor FGV_07 (9/16)\n",
      "Reading sensor FGV_08 (10/16)\n",
      "Reading sensor FGV_08 (11/16)\n",
      "Reading sensor FGV_09 (12/16)\n",
      "Reading sensor FGV_09 (13/16)\n",
      "Reading sensor FGV_10 (14/16)\n",
      "Reading sensor FGV_10 (15/16)\n",
      "Reading sensor FGV_11 (16/16)\n",
      "0: 2025-02-12 15:50:00 (Sensor FGV_01)\n",
      "0: 2025-02-12 16:00:00 1: 2024-07-19 20:20:00 (Sensor FGV_02)\n",
      "0: 2025-02-12 16:10:00 1: 2024-07-19 20:30:00 (Sensor FGV_03)\n",
      "0: 2024-07-19 20:40:00 (Sensor FGV_04)\n",
      "0: 2024-07-19 20:50:00 (Sensor FGV_05)\n",
      "0: 2024-07-24 15:10:00 (Sensor FGV_06)\n",
      "0: 2024-07-24 12:40:00 (Sensor FGV_07)\n",
      "0: 2025-02-13 13:30:00 1: 2024-07-07 17:50:00 (Sensor FGV_08)\n",
      "0: 2025-02-13 13:30:00 1: 2024-07-07 18:10:00 (Sensor FGV_09)\n",
      "0: 2024-09-23 10:30:00 1: 2024-07-07 16:40:00 (Sensor FGV_10)\n",
      "0: 2024-11-24 11:50:00 (Sensor FGV_11)\n",
      "Finished in 20.28s\n"
     ]
    }
   ],
   "source": [
    "concat_sensor_files(path_to_files, save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "571073c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading old file...\n",
      "Done (0.67s). Concatenating new files...\n",
      "Found 16 files\n",
      "Reading sensor FGV_01 (1/16)\n",
      "Reading sensor FGV_02 (2/16)\n",
      "Reading sensor FGV_02 (3/16)\n",
      "Reading sensor FGV_03 (4/16)\n",
      "Reading sensor FGV_03 (5/16)\n",
      "Reading sensor FGV_04 (6/16)\n",
      "Reading sensor FGV_05 (7/16)\n",
      "Reading sensor FGV_06 (8/16)\n",
      "Reading sensor FGV_07 (9/16)\n",
      "Reading sensor FGV_08 (10/16)\n",
      "Reading sensor FGV_08 (11/16)\n",
      "Reading sensor FGV_09 (12/16)\n",
      "Reading sensor FGV_09 (13/16)\n",
      "Reading sensor FGV_10 (14/16)\n",
      "Reading sensor FGV_10 (15/16)\n",
      "Reading sensor FGV_11 (16/16)\n",
      "Done (16.64s). Combining...\n",
      "Done (16.93s). Saving...\n",
      "Done combining files, took 21.51s\n"
     ]
    }
   ],
   "source": [
    "def append_sensor_files(path_to_files: str, old_file: str, save_path: str):\n",
    "    \"\"\"Concatenate an existing file with new ones.\"\"\"\n",
    "    stime = time.perf_counter()\n",
    "    print(\"Reading old file...\")\n",
    "    base = pd.read_csv(old_file)\n",
    "    print(f\"Done ({time.perf_counter()-stime:.2f}s). Concatenating new files...\")\n",
    "    new = concat_sensor_files(path_to_files=path_to_files)\n",
    "    print(f\"Done ({time.perf_counter()-stime:.2f}s). Combining...\")\n",
    "    combined = pd.concat([new, base])\n",
    "    print(f\"Done ({time.perf_counter()-stime:.2f}s). Saving...\")\n",
    "    with open(save_path, \"w\") as f:\n",
    "        combined.to_csv(f, index=False)\n",
    "\n",
    "    print(f\"Done combining files, took {time.perf_counter()-stime:.2f}s\")\n",
    "\n",
    "append_sensor_files(path_to_files, save_path, \"all_duplicate.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1d9c1c0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate all files in given list\n",
    "\n",
    "import pandas as pd\n",
    "import glob\n",
    "import re\n",
    "import time\n",
    "\n",
    "# path_to_files = r\"C:\\Users\\kevin\\Documents\\Code\\angelverein\\Daten FGV\"\n",
    "\n",
    "def concat_sensor_files_testfunc(path_to_files: str |list[str], save_path=None) -> None | pd.DataFrame :\n",
    "    \"\"\"Concatenate all given csv files collected from sensors into new ones.\"\"\"\n",
    "    if type(path_to_files) is list: \n",
    "        print(f\"Selected files: {path_to_files}\")\n",
    "        data_paths = path_to_files\n",
    "    else:\n",
    "        data_paths = glob.glob(path_to_files+\"\\\\FGV_*.xlsx\", recursive=True)\n",
    "        print(\"Found\", len(data_paths), \"files\")\n",
    "\n",
    "        # sensors_chunks = {key: [] for key in self.__config.sensors}\n",
    "    sensors_chunks = {}\n",
    "\n",
    "    if save_path is not None: stime = time.perf_counter()\n",
    "\n",
    "    for idx, file in enumerate(data_paths):\n",
    "        sensor_name = re.search(r\"FGV_\\d+\", file).group()\n",
    "        if not sensor_name in sensors.keys():\n",
    "            raise(NameError(f\"Trying to read sensor {sensor_name} which is not defined in sensors.toml\"))\n",
    "        if sensor_name not in sensors_chunks.keys():\n",
    "                sensors_chunks[sensor_name] = []\n",
    "        print(\"Reading sensor\", sensor_name, f\"({idx+1}/{len(data_paths)})\")\n",
    "        df = pd.read_excel(file)\n",
    "\n",
    "        idxcol, timecol, tmpcol = None, None, None\n",
    "        for idx, col in enumerate(df.columns):\n",
    "            # print(idx, col)\n",
    "            if config[\"names\"][\"index_column\"] in col:\n",
    "                idxcol = col\n",
    "                # print(\"Found index column:\", col)\n",
    "            elif config[\"names\"][\"timestamp_column\"] in col:\n",
    "                timecol = col\n",
    "                # print(\"Found timestamp column:\", col)\n",
    "            elif config[\"names\"][\"temperature_column\"] in col:\n",
    "                tmpcol = col\n",
    "                # print(\"Found temperature column:\", col)\n",
    "            else: raise(IndexError(f\"Found unknown column: {col}\"))\n",
    "\n",
    "        df[timecol] = pd.to_datetime(df[timecol], format=config[\"formats\"][\"time_format\"])\n",
    "        df.sort_values(timecol, ascending=False, inplace=True)\n",
    "        df.dropna()\n",
    "        # sensors_chunks[sensor_name].append({\"df\": df, \"idxcol\": idxcol, \"timecol\": timecol, \"tmpcol\": tmpcol})\n",
    "        df = transformSensorFile({\"df\": df, \"idxcol\": idxcol, \"timecol\": timecol, \"tmpcol\": tmpcol}, sensor_name, datetime_col=True)[\"df\"]\n",
    "        sensors_chunks[sensor_name].append(df)\n",
    "\n",
    "    # Use topmost entry\n",
    "    searchfunc = lambda x: x[\"Datum\"].iloc[0]\n",
    "    # Sort chunks after newest newest entry\n",
    "    for key in sensors_chunks.keys():\n",
    "        sensors_chunks[key].sort(key=searchfunc, reverse=True) # Newest at top\n",
    "        for idx, x in enumerate(sensors_chunks[key]):\n",
    "            if save_path is not None: print(f\"{idx}: {searchfunc(x)}\", end=\" \")\n",
    "        if save_path is not None: print(f\"(Sensor {key})\")\n",
    "        sensors_chunks[key] = pd.concat(sensors_chunks[key])\n",
    "\n",
    "    all_sensors_chunks = [sensors_chunks[key] for key in sensors_chunks.keys()]\n",
    "    all_sensors_chunks = pd.concat(all_sensors_chunks)\n",
    "    # sensors_chunks should now contain all the sensor files, sorted for all sensors read with newest data at the top\n",
    "\n",
    "    if save_path is not None:\n",
    "        with open(save_path, \"w\") as f:\n",
    "            all_sensors_chunks.to_csv(f, index=False)\n",
    "\n",
    "        ttime = time.perf_counter() - stime\n",
    "        print(f\"Finished in {ttime:.2f}s\")\n",
    "\n",
    "    else: return all_sensors_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4d582c2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected files: ['C:\\\\Users\\\\kevin\\\\Documents\\\\Code\\\\angelverein\\\\Daten FGV\\\\FGV_03 2024-07-19 20_34_15 CEST (Data CEST).xlsx', 'C:\\\\Users\\\\kevin\\\\Documents\\\\Code\\\\angelverein\\\\Daten FGV\\\\FGV_08 2025-02-13 13_28_34 CET (Data CET).xlsx']\n",
      "Reading sensor FGV_03 (1/2)\n",
      "Reading sensor FGV_08 (2/2)\n",
      "0: 2024-07-19 20:30:00 (Sensor FGV_03)\n",
      "0: 2025-02-13 13:30:00 (Sensor FGV_08)\n",
      "Finished in 3.12s\n"
     ]
    }
   ],
   "source": [
    "path_to_files = [r\"C:\\Users\\kevin\\Documents\\Code\\angelverein\\Daten FGV\\FGV_03 2024-07-19 20_34_15 CEST (Data CEST).xlsx\", r\"C:\\Users\\kevin\\Documents\\Code\\angelverein\\Daten FGV\\FGV_08 2025-02-13 13_28_34 CET (Data CET).xlsx\"]\n",
    "save_path = \"all_data_und.csv\"\n",
    "concat_sensor_files_testfunc(path_to_files, save_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
